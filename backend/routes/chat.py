from fastapi import APIRouter, HTTPException  # âœ… APIRouter import ì¶”ê°€
from pydantic import BaseModel
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

router = APIRouter()  # âœ… APIRouter ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

# âœ… ChatRequest ëª¨ë¸ ì •ì˜ (ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë°›ì„ ë°ì´í„° êµ¬ì¡°)
class ChatRequest(BaseModel):
    prompt: str

# ëª¨ë¸ ë¡œë“œ
MODEL_NAME = "LGAI-EXAONE/EXAONE-Deep-2.4B"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

device = "mps" if torch.backends.mps.is_available() else "cpu"
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, trust_remote_code=True)
model.to(device)

@router.post("/chat")
async def chat(request: ChatRequest):
    print(f"ğŸ“© Received prompt: {request.prompt}")

    try:
        # âœ… ëª¨ë¸ì´ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µí•  ìˆ˜ ìˆë„ë¡ í”„ë¡¬í”„íŠ¸ ì •ë¦¬
        prompt = f"User: {request.prompt}\nAI:"

        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        print("ğŸ›  Generating response...")

        output = model.generate(
            **inputs, 
            max_length=100,
            temperature=1.0,
            top_k=50,
            top_p=0.9,
            do_sample=True,
            repetition_penalty=1.2,
        )

        response = tokenizer.decode(output[0], skip_special_tokens=True)
        
        # âœ… AIì˜ ì‘ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        if "AI:" in response:
            response = response.split("AI:")[-1].strip()

        print(f"âœ… Generated response: {response}")  
        return {"response": response}

    except Exception as e:
        print(f"âŒ Error during response generation: {e}")
        return {"error": "Failed to generate response"}
